name: Preprod Snapshot Creation

on:
  # Manual trigger
  workflow_dispatch:
    inputs:
      skip_sync_check:
        description: 'Skip indexer sync check (use with caution)'
        required: false
        type: boolean
        default: false
  
  # Scheduled: Daily at midnight UTC
  schedule:
    - cron: '0 0 * * *'

# Permissions needed for GitHub Pages deployment and PR comments
permissions:
  contents: write  # Changed from 'read' to 'write' to allow pushing to gh-pages branch
  pages: write
  id-token: write
  pull-requests: write

env:
  DB_HOST: localhost
  DB_PORT: 5432
  DB_NAME: rosetta-java
  DB_USER: rosetta_db_admin
  DB_SCHEMA: preprod
  NETWORK: preprod
  SNAPSHOT_DIR: /home/integration/git/cardano-rosetta-java/snapshots
  API_HOST: localhost
  API_PORT: 8082
  COMPOSE_PROJECT_NAME: cardano-rosetta-java
  YACI_CONTAINER: cardano-rosetta-java-yaci-indexer-1

jobs:
  check-sync-status:
    name: Check Indexer Sync Status
    runs-on: [preprod-snapshot-builder]
    outputs:
      is_synced: ${{ steps.sync-check.outputs.synced }}
    
    steps:
      - name: Check indexer is running
        run: |
          if ! docker ps --format '{{.Names}}' | grep -q "^${YACI_CONTAINER}$"; then
            echo "‚ùå ERROR: Indexer container '${YACI_CONTAINER}' is not running"
            exit 1
          fi
          echo "‚úÖ Indexer container is running"
      
      - name: Wait for indexer to sync
        id: sync-check
        if: ${{ !inputs.skip_sync_check }}
        run: |
          echo "üîç Checking indexer sync status..."
          MAX_ATTEMPTS=12  # Check every 5 minutes for 1 hour
          ATTEMPT=0
          
          while [ $ATTEMPT -lt $MAX_ATTEMPTS ]; do
            RESPONSE=$(curl -s --location "${API_HOST}:${API_PORT}/network/status" \
              --header 'Content-Type: application/json' \
              --data "{
                \"network_identifier\": {
                  \"blockchain\": \"cardano\",
                  \"network\": \"${NETWORK}\"
                },
                \"metadata\": {}
              }" 2>/dev/null || echo "{}")
            
            if [ -z "$RESPONSE" ]; then
              echo "‚ö†Ô∏è  Could not connect to API at ${API_HOST}:${API_PORT}"
              ATTEMPT=$((ATTEMPT + 1))
              echo "Attempt $ATTEMPT/$MAX_ATTEMPTS - waiting 5 minutes..."
              sleep 300
              continue
            fi
            
            SYNCED=$(echo "$RESPONSE" | grep -o '"synced":[^,}]*' | sed 's/"synced"://' | tr -d ' ')
            
            if [ "$SYNCED" = "true" ]; then
              echo "‚úÖ Indexer is synced and at tip"
              echo "synced=true" >> $GITHUB_OUTPUT
              exit 0
            fi
            
            ATTEMPT=$((ATTEMPT + 1))
            echo "‚è≥ Indexer not synced yet (Attempt $ATTEMPT/$MAX_ATTEMPTS)"
            if [ $ATTEMPT -lt $MAX_ATTEMPTS ]; then
              echo "Waiting 5 minutes before next check..."
              sleep 300
            fi
          done
          
          echo "‚ùå ERROR: Indexer not synced after 1 hour"
          echo "synced=false" >> $GITHUB_OUTPUT
          exit 1

  prepare-snapshot:
    name: Prepare Snapshot Metadata
    runs-on: [preprod-snapshot-builder]
    needs: check-sync-status
    outputs:
      block_number: ${{ steps.metadata.outputs.block_number }}
      slot_number: ${{ steps.metadata.outputs.slot_number }}
      snapshot_name: ${{ steps.metadata.outputs.snapshot_name }}
      date_folder: ${{ steps.metadata.outputs.date_folder }}
    
    steps:
      - name: Pause yaci-indexer
        id: pause-indexer
        run: |
          echo "‚è∏Ô∏è  Pausing yaci-indexer to ensure consistent snapshot..."
          if docker pause "${YACI_CONTAINER}" 2>&1; then
            echo "‚úÖ Indexer paused"
            echo "paused=true" >> $GITHUB_OUTPUT
          else
            echo "‚ö†Ô∏è  Could not pause indexer - continuing anyway"
            echo "paused=false" >> $GITHUB_OUTPUT
          fi
      
      - name: Collect database metadata
        id: metadata
        env:
          PGPASSWORD: ${{ secrets.PREPROD_DB_SECRET }}
        run: |
          echo "üìä Collecting metadata from database..."
          
          BLOCK_NUMBER=$(psql -h ${DB_HOST} -p ${DB_PORT} -U ${DB_USER} -d ${DB_NAME} -t -c \
            "SELECT COALESCE(MAX(number), 0) FROM ${DB_SCHEMA}.block;" | xargs)
          
          SLOT_NUMBER=$(psql -h ${DB_HOST} -p ${DB_PORT} -U ${DB_USER} -d ${DB_NAME} -t -c \
            "SELECT COALESCE(MAX(slot), 0) FROM ${DB_SCHEMA}.block;" | xargs)
          
          TABLE_COUNT=$(psql -h ${DB_HOST} -p ${DB_PORT} -U ${DB_USER} -d ${DB_NAME} -t -c \
            "SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = '${DB_SCHEMA}';" | xargs)
          
          TIMESTAMP=$(date +%Y%m%d_%H%M%S)
          DATE_FOLDER=$(date +%Y-%m-%d)
          SNAPSHOT_NAME="snapshot_${NETWORK}_${TIMESTAMP}-block${BLOCK_NUMBER}.dump"
          
          echo "   - Block Number: ${BLOCK_NUMBER}"
          echo "   - Slot Number: ${SLOT_NUMBER}"
          echo "   - Table Count: ${TABLE_COUNT}"
          echo "   - Snapshot Name: ${SNAPSHOT_NAME}"
          
          echo "block_number=${BLOCK_NUMBER}" >> $GITHUB_OUTPUT
          echo "slot_number=${SLOT_NUMBER}" >> $GITHUB_OUTPUT
          echo "table_count=${TABLE_COUNT}" >> $GITHUB_OUTPUT
          echo "snapshot_name=${SNAPSHOT_NAME}" >> $GITHUB_OUTPUT
          echo "date_folder=${DATE_FOLDER}" >> $GITHUB_OUTPUT
          echo "timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)" >> $GITHUB_OUTPUT


  create-dump:
    name: Create PostgreSQL Dump
    runs-on: [preprod-snapshot-builder]
    needs: prepare-snapshot
    outputs:
      snapshot_file: ${{ steps.dump.outputs.SNAPSHOT_FILE }}
      snapshot_size_mb: ${{ steps.dump.outputs.SNAPSHOT_SIZE_MB }}
      dump_duration_min: ${{ steps.dump.outputs.DUMP_DURATION_MIN }}

    steps:
      - name: Create database dump
        id: dump
        env:
          PGPASSWORD: ${{ secrets.PREPROD_DB_SECRET }}
          DB_SECRET: ${{ secrets.PREPROD_DB_SECRET }}
        run: |
          echo "üöÄ Creating PostgreSQL dump..."

          # Use absolute path to script on the self-hosted runner
          SCRIPT_PATH="/home/integration/git/cardano-rosetta-java/docker/dockerfiles/postgres/create-snapshot.sh"

          if [ ! -f "$SCRIPT_PATH" ]; then
            echo "‚ùå ERROR: Script not found at $SCRIPT_PATH"
            exit 1
          fi

          docker run --rm \
            --network cardano-rosetta-java-preprod \
            -e DB_HOST=${DB_HOST} \
            -e DB_PORT=${DB_PORT} \
            -e DB_NAME=${DB_NAME} \
            -e DB_USER=${DB_USER} \
            -e DB_SECRET=${{ secrets.PREPROD_DB_SECRET }} \
            -e DB_SCHEMA=${DB_SCHEMA} \
            -e NETWORK=${NETWORK} \
            -e SNAPSHOT_DIR=${SNAPSHOT_DIR} \
            -e GITHUB_OUTPUT=/tmp/github_output \
            -v ${SNAPSHOT_DIR}:${SNAPSHOT_DIR} \
            -v ${SCRIPT_PATH}:/create-snapshot.sh:ro \
            cardanofoundation/cardano-rosetta-java-postgres:latest \
            bash /create-snapshot.sh \
              --block-number "${{ needs.prepare-snapshot.outputs.block_number }}" \
              --snapshot-name "${{ needs.prepare-snapshot.outputs.snapshot_name }}"

          # Verify file exists and capture metadata
          SNAPSHOT_FILE="${SNAPSHOT_DIR}/${{ needs.prepare-snapshot.outputs.date_folder }}/${{ needs.prepare-snapshot.outputs.snapshot_name }}"
          if [ -f "$SNAPSHOT_FILE" ]; then
            FILE_SIZE=$(stat -c%s "$SNAPSHOT_FILE" 2>/dev/null || stat -f%z "$SNAPSHOT_FILE" 2>/dev/null)
            SIZE_MB=$((FILE_SIZE / 1024 / 1024))
            echo "SNAPSHOT_FILE=${SNAPSHOT_FILE}" >> $GITHUB_OUTPUT
            echo "SNAPSHOT_SIZE_MB=${SIZE_MB}" >> $GITHUB_OUTPUT
            echo "‚úÖ Dump created: ${SNAPSHOT_FILE} (${SIZE_MB} MB)"
          else
            echo "‚ùå ERROR: Dump file not found"
            exit 1
          fi

  generate-metadata:
    name: Generate Metadata Files
    runs-on: [preprod-snapshot-builder]
    needs: [prepare-snapshot, create-dump]

    steps:
      - name: Generate checksum
        run: |
          echo "üîê Generating SHA256 checksum..."
          SNAPSHOT_FILE="${SNAPSHOT_DIR}/${{ needs.prepare-snapshot.outputs.date_folder }}/${{ needs.prepare-snapshot.outputs.snapshot_name }}"
          CHECKSUM_FILE="${SNAPSHOT_FILE%.dump}.checksum"

          cd "$(dirname "$SNAPSHOT_FILE")"
          sha256sum "$(basename "$SNAPSHOT_FILE")" > "$(basename "$CHECKSUM_FILE")"

          echo "‚úÖ Checksum created: $(basename "$CHECKSUM_FILE")"
          cat "$CHECKSUM_FILE"

      - name: Generate metadata JSON
        run: |
          echo "üìã Generating metadata JSON..."
          SNAPSHOT_FILE="${SNAPSHOT_DIR}/${{ needs.prepare-snapshot.outputs.date_folder }}/${{ needs.prepare-snapshot.outputs.snapshot_name }}"
          METADATA_FILE="${SNAPSHOT_FILE%.dump}.metadata.json"
          CHECKSUM=$(cat "${SNAPSHOT_FILE%.dump}.checksum" | awk '{print $1}')

          cat > "$METADATA_FILE" << 'METADATA_EOF'
          {
            "snapshot": {
              "name": "${{ needs.prepare-snapshot.outputs.snapshot_name }}",
              "created_at": "${{ needs.prepare-snapshot.outputs.timestamp }}",
              "format": "pg_dump custom format",
              "size_bytes": SIZE_PLACEHOLDER,
              "checksum_sha256": "CHECKSUM_PLACEHOLDER",
              "compressed": true,
              "compression_level": 9
            },
            "source": {
              "network": "${NETWORK}",
              "database": {
                "host": "${DB_HOST}",
                "port": ${DB_PORT},
                "name": "${DB_NAME}",
                "schema": "${DB_SCHEMA}",
                "table_count": ${{ needs.prepare-snapshot.outputs.table_count }},
                "highest_block": ${{ needs.prepare-snapshot.outputs.block_number }},
                "highest_slot": ${{ needs.prepare-snapshot.outputs.slot_number }}
              }
            }
          }
          METADATA_EOF

          # Replace placeholders
          FILE_SIZE=$(stat -c%s "$SNAPSHOT_FILE" 2>/dev/null || stat -f%z "$SNAPSHOT_FILE" 2>/dev/null)
          sed -i "s/SIZE_PLACEHOLDER/${FILE_SIZE}/g" "$METADATA_FILE"
          sed -i "s/CHECKSUM_PLACEHOLDER/${CHECKSUM}/g" "$METADATA_FILE"

          echo "‚úÖ Metadata created: $(basename "$METADATA_FILE")"
          cat "$METADATA_FILE"

  cleanup-and-resume:
    name: Cleanup and Resume Indexer
    runs-on: [preprod-snapshot-builder]
    needs: [prepare-snapshot, create-dump, generate-metadata]
    if: always()

    steps:
      - name: Resume yaci-indexer
        if: always()
        run: |
          echo "‚ñ∂Ô∏è  Resuming yaci-indexer..."
          if docker unpause "${YACI_CONTAINER}" 2>&1; then
            echo "‚úÖ Indexer resumed"
            sleep 5
            if docker ps --format '{{.Names}}\t{{.Status}}' | grep "${YACI_CONTAINER}"; then
              echo "‚úÖ Indexer is running normally"
            else
              echo "‚ö†Ô∏è  WARNING: Indexer may not be running properly"
            fi
          else
            echo "‚ö†Ô∏è  Could not unpause indexer"
            echo "Please manually resume: docker unpause ${YACI_CONTAINER}"
          fi

      - name: Clean up old snapshots
        run: |
          echo "üßπ Cleaning up snapshots older than 7 days..."
          find ${SNAPSHOT_DIR} -type d -mtime +7 -exec rm -rf {} + 2>/dev/null || true

          echo "üìä Current snapshots:"
          du -sh ${SNAPSHOT_DIR}/*/ 2>/dev/null || echo "No snapshot directories found"

      - name: Summary
        if: success()
        run: |
          echo "‚úÖ Snapshot creation completed successfully!"
          echo ""
          echo "üì¶ Snapshot Details:"
          echo "   - Name: ${{ needs.prepare-snapshot.outputs.snapshot_name }}"
          echo "   - Block: ${{ needs.prepare-snapshot.outputs.block_number }}"
          echo "   - Size: ${{ needs.create-dump.outputs.snapshot_size_mb }} MB"
          echo "   - Location: ${SNAPSHOT_DIR}/${{ needs.prepare-snapshot.outputs.date_folder }}/"
          echo ""
          echo "üìÅ Files created:"
          ls -lh "${SNAPSHOT_DIR}/${{ needs.prepare-snapshot.outputs.date_folder }}/" | tail -n +2
